---
marp: true
theme: default
header: ''
footer: 'Kosuke Toda ([@SeeKT](https://github.com/SeeKT))'

size: 16:9
---
<!-- paginate: true -->
# サポートベクトルマシン 5.3節
#### 参考文献
- [金森, 統計的学習理論, 講談社, 2015.](https://sites.google.com/site/tokyotechkanamoritakafumilab/)
#### Table of contents
- $C$-サポートベクトルマシン

---
### $C$-サポートベクトルマシン
ヒンジ損失を用いて判別関数$f(x) + b$を学習する．
$$
\text{minimize}_{f, b} \ \  C \sum_{i = 1}^n \phi_{\text{hinge}}(y_i(f(x_i) + b)) + \frac{1}{2}\|f\|_{\mathcal{H}}^2 \\
\text{subject to} \ \ f \in \mathcal{H}, \ b \in \mathbb{R} \ \ (5.1)
$$
- $C > 0$: ヒンジ損失と正則化項のバランスを調整する正則化パラメータ

この最適化問題を解くことで判別関数が得られる．
データ数に応じて$C$を適切に制御することで，高い予測精度を持つ判別器を得ることができる．

---
$\xi$: スラック変数 を用いて，ヒンジ損失を書き直す．
$$
\phi_{\text{hinge}}(m) = \min \{\xi \in \mathbb{R} \, | \, \xi \geq 0, \xi \geq 1 - m\}
$$
Th4.10より，関数$f$の最適解は適当な係数$\alpha_1, \ldots, \alpha_n$を用いて$\sum_{i = 1}^n \alpha_i k(x_i, \cdot)$と表せる $\rightsquigarrow$ (5.1)に代入して整理．
$$
\text{minimize}_{a, b, \xi} \ \ C \sum_{i = 1}^n \xi_i + \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j K(x_i, x_j) \\
\text{subject to} \ \ \xi_i \geq 0, \xi_i \geq 1 - y_i \left( \sum_{j = 1}^n \alpha_j K(x_j, x_i) + b \right), \ i = 1, \ldots, n \\
\alpha_1, \ldots, \alpha_n, b, \xi_i, \ldots, \xi_n \in \mathbb{R} \ \ (5.2)
$$

これは凸最適化問題．

---

### $C$-サポートベクトルマシンの最適性条件
(5.2)の制約式では，$\xi_i$が十分大きな値をとれば，スレイター制約想定が満たされる
$\rightsquigarrow$ ミニマックス定理より，強双対性が成り立ち，(5.2)に対する双対問題を導出できる

ラグランジュ関数を
$$
L(\alpha, b, \xi; \gamma, \delta) = C \sum_i \xi_i + \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j K_{ij} - \sum_i \delta_i \xi_i \\
+ \sum_i \gamma_i \left(1 - y_i \left( \sum_{j = 1}^n \alpha_j K_{ij} + b  \right) - \xi_i \right)
$$
と定義する ($\gamma_i, \delta_i$は非負のラグランジュ定数) ．

---
強双対性より，双対問題が求められ，
$$
\text{maximize}_{\gamma_1, \ldots, \gamma_n} \ \ - \frac{1}{2} \sum_{i,j} \gamma_i \gamma_j y_i y_j K_{ij} + \sum_i \gamma_i \\
\text{subject to} \ \ \sum_i \gamma_i y_i = 0, \ 0 \leq \gamma_i \leq C, \ i = 1, \ldots, n \ \ (5.3)
$$
となる (導出略)．

最適性条件より，主問題の最適解$\alpha_i$は双対問題の最適解$\gamma_i$から
$$
\alpha_i = y_i \gamma_i, \ \ i = 1, \ldots, n
$$
によって得られる．