---
marp: true
theme: default
header: ''
footer: 'Kosuke Toda ([@SeeKT](https://github.com/SeeKT))'

size: 16:9
---
<!-- paginate: true -->
# 仮説集合の複雑度 2.1節
#### 参考文献
- [金森, 統計的学習理論, 講談社, 2015.](https://sites.google.com/site/tokyotechkanamoritakafumilab/)
#### Table of contents
- VC次元
---

### VC次元
- VC dimension
    - Vapnik and Chervonenkis
    - 2値判別問題のための仮説集合に対して定義される複雑度
        - 多値判別や回帰問題の場合に拡張することも可能．
    - 集合族の組合せ的な性質を捉えるための量
        - 組合せ論などにも応用

---

- $\mathcal{H}$: 2値判別のための仮説集合
    - $h \in \mathcal{H}$: 仮説 ($h: \mathcal{X} \to \mathcal{Y}, \ |\mathcal{Y}| = 2$)

$\{x_1, \ldots, x_n\} \subset \mathcal{X}$: 入力の集合 に対して，$\mathcal{Y}^n$の部分集合
$$
\{(h(x_1), \ldots, h(x_n)) \in \mathcal{Y}^n \, | \, h \in \mathcal{H}\}
$$
の要素数を
$$
\Pi_{\mathcal{H}}(x_1, \ldots, x_n) = |\{(h(x_1), \ldots, h(x_n)) \in \mathcal{Y}^n \, | \, h \in \mathcal{H}\}|
$$
とおく．定義から，
$$
\Pi_{\mathcal{H}}(x_1, \ldots, x_n) \leq 2^n
$$
が成り立つ．

---

等式$\Pi_{\mathcal{H}}(x_1, \ldots, x_n) = 2^n$ が成り立つとする．
$\rightsquigarrow$ 各$x_i$にラベル$y_i \in \mathcal{Y}$を割り付けて得られる任意の2値データ$\{(x_1, y_1), \ldots, (x_n, y_n)\}$に対して，適切に$h \in \mathcal{H}$を選べば
$$
h(x_i) = y_i, \ i = 1, \ldots, n
$$
とできる．

入力の数$n$が増えていけば，ラベル付けのパターンが豊富になり，上記の等式が成立しにくくなると考えられる．
その境界となるデータ数$n$を$\mathcal{H}$のVC次元という．つまり，
$$
\mathrm{VCdim}(\mathcal{H}) = \max \left\{ n \in \mathbb{N} \, \left| \, \max_{x_1, \ldots, x_n \in \mathcal{X}} \Pi_{\mathcal{H}}(x_1, \ldots, x_n) = 2^n \right. \right\}
$$
と定義される．
$\mathrm{VCdim}(\mathcal{H}) = \infty$ $\overset{\text{def}}{\Longleftrightarrow}$ $\forall n \in \mathbb{N}, \ \exists x_1, \ldots, x_n \in \mathcal{X}, \ \Pi_{\mathcal{H}}(x_1, \ldots, x_n) = 2^n$.

---

- VC次元の解釈
    - どのようなラベル付けにも対応可能な仮説が存在するようなデータ数の上限
- データ数$n$が$n \leq \mathrm{VCdim}(\mathcal{H})$のときは学習がうまくいくと考えることもできる．
    - ノイズによってラベルが反転してしまう状況を考えると，必ずしも学習がうまくいくわけではない
    - 仮説集合がどのようなラベル付けにも対応できる = ノイズとして無視すべきデータも学習してしまう
    - 仮説集合の複雑度は，データの複雑さに合わせて適切に設定することが重要

---
###### Lemma 2.1 (Sauer's lemma)
2値ラベルに値をとる仮説集合$\mathcal{H}$のVC次元が$d$のとき，$n \geq d$に対して
$$
\max_{x_1, \ldots, x_n \in \mathcal{X}} \Pi_{\mathcal{H}}(x_1, \ldots, x_n) \leq \left( \frac{en}{d} \right)^d \ \ (e: \text{ネイピア数})
$$

###### Th 2.2
2値ラベルに値をとる仮説集合$\mathcal{H} \subset \{h: \mathcal{X} \to \{-1, +1\}\}$のVC次元を$d < \infty$とする．学習データ$(X_1, Y_1), \ldots, (X_n, Y_n)$はi.i.d.であるとする．損失として0-1損失を用いると，$n \geq d$のとき，学習データの分布の下で確率$1 - \delta$以上で
$$
\sup_{h \in \mathcal{H}} |R_{\mathrm{err}}(h) - \hat{R}_{\mathrm{err}}(h)| \leq 2 \sqrt{\frac{2d}{n} \log \frac{en}{d}} + \sqrt{\frac{\log(2/\delta)}{2n}}
$$
が成り立つ．

---
Lemma 2.1の証明は略．Th 2.2の証明は2.3節で示す．

Th 2.2を用いて，推定された仮説の予測判別誤差を評価する．
$\rightsquigarrow$ 有限集合ではない仮説集合が扱える．

- $S = \{(X_1, Y_1), \ldots, (X_n, Y_n)\}$: 学習データ
- $h_S$: 経験判別誤差$\hat{R}_{\mathrm{err}}(h)$の最小化によって得られる仮説
- $h_0$: ベイズ規則
    - 予測損失$R_{\mathrm{err}}(h)$の下限を達成する仮説
    - $h_0 \in \mathcal{H}$を仮定．

$\rightsquigarrow$ 以下の不等式が常に成立
$$
\hat{R}_{\mathrm{err}}(h_S) \leq \hat{R}_{\mathrm{err}}(h_0), \ \ R_{\mathrm{err}}(h_0) \leq R_{\mathrm{err}}(h_S)
$$

---
$\rightsquigarrow$ 学習データ$S$の分布の下で，$1 - \delta$以上の確率で以下が成立．
$
R_{\mathrm{err}}(h_S) \leq \hat{R}_{\mathrm{err}}(h_0) + R_{\mathrm{err}}(h_S) - \hat{R}_{\mathrm{err}}(h_S) \ \ (\because \hat{R}_{\mathrm{err}}(h_0) \geq \hat{R}_{\mathrm{err}}(h_S))
$
$
\leq R_{\mathrm{err}}(h_0) + |R_{\mathrm{err}}(h_0) + \hat{R}_{\mathrm{err}}(h_0)| + \sup_{h \in \mathcal{H}} |R_{\mathrm{err}}(h) - \hat{R}_{\mathrm{err}}(h)| \ \ (\because (\dagger), (\ddagger))
$
$
\leq R_{\mathrm{err}}(h_0) + 2 \sup_{h \in \mathcal{H}} |R_{\mathrm{err}}(h) - \hat{R}_{\mathrm{err}}(h)| \ \ (\because (\ddagger))
$
$
\leq R_{\mathrm{err}}(h_0) + 4\sqrt{\frac{2d}{n} \log \frac{en}{d}} + 2\sqrt{\frac{\log(2/\delta)}{2n}} \ \ (\because \text{Th }2.2)
$

これより，
$$
R_{\mathrm{err}}(h_0) \leq R_{\mathrm{err}}(h_S) \leq R_{\mathrm{err}}(h_0) + O_p\left( \sqrt{\frac{\log(n/d)}{n/d}} \right)
$$
となる．
$\rightsquigarrow$ 予測判別誤差はデータ数とVC次元の比$n/d$と関連している．

---
($\dagger$)
$
\hat{R}_{\mathrm{err}}(h_0) = R_{\mathrm{err}}(h_0) - R_{\mathrm{err}}(h_0) + \hat{R}_{\mathrm{err}}(h_0)
$
$
\leq R_{\mathrm{err}}(h_0) + |R_{\mathrm{err}}(h_0) - \hat{R}_{\mathrm{err}}(h_0)|
$

($\ddagger$)
$
R_{\mathrm{err}}(h_S) - \hat{R}_{\mathrm{err}}(h_S) \leq |R_{\mathrm{err}}(h_S) - \hat{R}_{\mathrm{err}}(h_0)|
$
$
\leq \sup_{h \in \mathcal{H}} |R_{\mathrm{err}}(h) - \hat{R}_{\mathrm{err}}(h)|
$

以下，VC次元の例を示す．

---
###### 例 2.1 (有限仮説集合)
有限な仮説集合$\mathcal{H}$に対して
$$
\mathrm{VCdim}(\mathcal{H}) \leq \log_2 |\mathcal{H}|
$$
が成り立つ．
- $d$個の入力点に割り当てられるラベルのパターンは$2^d$通り
    - $|\mathcal{H}| < 2^d$ならば，すべてのラベルの割り当てに対応することができない

$\rightsquigarrow$ 推定された仮説$h_S$の予測損失について
$$
R_{\mathrm{err}}(h_0) \leq R_{\mathrm{err}}(h_S) \leq R_{\mathrm{err}}(h_0) + O_p\left( \sqrt{\frac{\log |\mathcal{H}|}{n}} \log \frac{n}{\log |\mathcal{H}|} \right)
$$
が成立する．

---
###### 例 2.2 (線形判別器のVC次元)
- $\mathcal{X} = \mathbb{R}^d$: 入力空間
- $\mathcal{H} = \{h(x) = \mathrm{sign}(w^{\mathrm{T}}x + b) \, | \, w \in \mathbb{R}^d, b \in \mathbb{R}\}$: 仮説集合 (線形判別器の集合)
- $\{x_1, \ldots, x_{d + 1}\} \subset \mathbb{R}^d$: 列ベクトルの集合

これらのベクトルが$\mathbb{R}^d$で一般の位置にあるとき
$$
A = \left(
    \begin{array}{ccc}
        x_1 & \cdots & x_{d + 1} \\
        1 & \cdots & 1
    \end{array}
    \right) \in \mathbb{R}^{(d + 1) \times (d + 1)}
$$
は可逆な行列となる．
入力$x_i$にラベル$y_i \in \{+1, -1\}$を割り当てたデータに対して
$$
\left(
    \begin{array}{c}
        w \\
        b
    \end{array}
    \right) = A^{-1}y, \ y = (y_1, \ldots, y_{d + 1})^{\mathrm{T}}
$$
とすれば，$h(x_i) = y_i, \ i = 1, \ldots, n$が成立．

---
実際，

$$
y = A \left(
        \begin{array}{c}
            w \\
            b
        \end{array}
    \right) = (w^{\mathrm{T}}x_1 + b, \ldots, w^{\mathrm{T}}x_{d + 1} + b)^{\mathrm{T}}
$$
より，$y_i = w^{\mathrm{T}}x_i + b = \mathrm{sign}(w^{\mathrm{T}}x_i + b)$となる．

$\rightsquigarrow$ $\mathrm{VCdim}(\mathcal{H}) \geq d + 1$

以下，$VC$次元の上界を求めるのに役立つラドンの定理を紹介する．
集合$A$の凸包$\mathrm{conv}(A)$を
$$
\mathrm{conv}(A) = \left\{ \left. \sum_{i = 1}^n \alpha_i x_i \, \right|  \, n \in \mathbb{N}, \sum_{i = 1}^n \alpha_i = 1, \alpha_i \in [0, 1], x_i \in A \right\}
$$
とする．